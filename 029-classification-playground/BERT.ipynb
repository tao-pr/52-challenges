{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "million-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-madness",
   "metadata": {},
   "source": [
    "# 1. Load BERT Model\n",
    "\n",
    "Pretrained multi-lingual from HuggingFace\n",
    "\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "random-latitude",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Multi-lingual\n",
    "mbertTokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mbertModel = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mbertModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-omaha",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mbertModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-lightweight",
   "metadata": {},
   "source": [
    "## Test encoding text in various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selected-shade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[ 101, 2026, 2047, 2338, 2003, 2417,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'My new book is red',\n",
    "    'The journal has been approved by the dean!',\n",
    "    'It was a dangerous explosion caused by chemical reaction',\n",
    "    'Potassium Nitrate'\n",
    "]\n",
    "\n",
    "tensors = [mbertTokeniser(s, return_tensors='tf') for s in sentences]\n",
    "\n",
    "tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "white-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "wouts = [mbertModel(t) for t in tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "declared-consumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'pooler_output']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wouts[0]] # output structure of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "planned-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 7, 768), (1, 11, 768), (1, 11, 768), (1, 4, 768)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w[0].numpy().shape for w in wouts] # sequence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exterior-basement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 768), (1, 768), (1, 768), (1, 768)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w[1].numpy().shape for w in wouts] # CLS embedding (pooled) output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "innovative-vegetable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 768), (1, 768), (1, 768), (1, 768)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten hidden states of N words in each sentence\n",
    "# by averaging\n",
    "fseqs = [w[1].numpy() for w in wouts]\n",
    "list(map(lambda f: f.shape, fseqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spoken-stand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'do', 'you', 'know', 'who', 'i', 'am', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = mbertTokeniser.encode('Do you know who i am') # str -> ids\n",
    "mbertTokeniser.convert_ids_to_tokens(encoded) # ids -> tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-cameroon",
   "metadata": {},
   "source": [
    "Distance between each pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "precise-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "zs = list(zip(sentences, fseqs))\n",
    "\n",
    "closest = []\n",
    "for i,(s1,v1) in enumerate(zs):\n",
    "    for j,(s2,v2) in enumerate(zs[i+1:]):\n",
    "        c = cosine_similarity(\n",
    "            np.atleast_2d(v1.flatten()),\n",
    "            np.atleast_2d(v2.flatten()))\n",
    "        heappush(closest, (-c, (s1,s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "governing-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 closest pairs\n",
      "==========================\n",
      "Rank #0, [[0.97097844]]\n",
      "It was a dangerous explosion caused by chemical reaction\n",
      "Potassium Nitrate\n",
      "==========================\n",
      "Rank #1, [[0.9686246]]\n",
      "The journal has been approved by the dean!\n",
      "It was a dangerous explosion caused by chemical reaction\n",
      "==========================\n",
      "Rank #2, [[0.9531851]]\n",
      "The journal has been approved by the dean!\n",
      "Potassium Nitrate\n",
      "==========================\n",
      "Rank #3, [[0.9341798]]\n",
      "My new book is red\n",
      "The journal has been approved by the dean!\n",
      "==========================\n",
      "Rank #4, [[0.8713379]]\n",
      "My new book is red\n",
      "It was a dangerous explosion caused by chemical reaction\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 closest pairs')\n",
    "for i in range(5):\n",
    "    c, (s1,s2) = heappop(closest)\n",
    "    print('==========================')\n",
    "    print(f'Rank #{i}, {-c}')\n",
    "    print(s1)\n",
    "    print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-guide",
   "metadata": {},
   "source": [
    "These sentence similarities make no sense at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-reliance",
   "metadata": {},
   "source": [
    "## Build Classification based on BERT\n",
    "\n",
    "By freezing pretrained layer of BERT, and add a new softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "political-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbertModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "closed-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7fd51673f610>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-visitor",
   "metadata": {},
   "source": [
    "Add custom smoothing & classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "composite-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "planned-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer (1) taking tokenised words\n",
    "input_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,), \n",
    "    dtype=tf.int32,\n",
    "    name=\"input_ids\")\n",
    "\n",
    "# input layer (2) taking attention masks (masking paddings)\n",
    "mask_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,),\n",
    "    dtype=tf.int32,\n",
    "    name=\"attention_mask_ids\")\n",
    "\n",
    "# input layer (3) taking token types\n",
    "token_type_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,),\n",
    "    dtype=tf.int32,\n",
    "    name=\"token_type_ids\")\n",
    "\n",
    "# BERT layer\n",
    "outputs = mbertModel(\n",
    "    input_ids,\n",
    "    attention_mask=mask_ids,\n",
    "    token_type_ids=token_type_ids)\n",
    "\n",
    "# smoothening layers\n",
    "dense = keras.layers.Dense(768, name=\"dense\")(outputs[1]) # feed BERT's pooled output\n",
    "dropout = keras.layers.Dropout(0.2, name=\"dropout\")(dense)\n",
    "norm = keras.layers.LayerNormalization(name=\"norm\")(dropout)\n",
    "logit = keras.layers.Dense(1, activation='sigmoid', name=\"logit\")(norm)\n",
    "\n",
    "# Modeling\n",
    "model = keras.models.Model(\n",
    "    inputs=[input_ids, mask_ids, token_type_ids],\n",
    "    outputs=[logit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "professional-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_ids (InputLayer) [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask_ids[0][0]         \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 768)          590592      tf_bert_model[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm (LayerNormalization)       (None, 768)          1536        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "logit (Dense)                   (None, 1)            769         norm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 110,075,137\n",
      "Trainable params: 592,897\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "shaped-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-president",
   "metadata": {},
   "source": [
    "## 2. Load tweets data\n",
    "For classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "acting-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-cached if exists\n",
    "path_cache = os.path.join(os.environ['HOME'], 'data', 'tweets', '_cache', 'bert-tokenised.pkl')\n",
    "if os.path.isfile(path_cache):\n",
    "    print(f'Loading pre-cached : {path_cache}')\n",
    "    tweets = pd.read_pickle(path_cache)\n",
    "else:\n",
    "    path_tweets = os.path.join(os.environ['HOME'], 'data', 'tweets', 'training.1600000.processed.noemoticon.csv')\n",
    "    cols = ['sentiment','user','tweet']\n",
    "    tweets = pd.read_csv(path_tweets, usecols=[0,4,5], header=None, index_col=None)\n",
    "    tweets.columns = cols\n",
    "    tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-integrity",
   "metadata": {},
   "source": [
    "Prepare text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "essential-bible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @_TheSpecialOne_ @switchfoot http://twitpic.co...\n",
       "1    @scotthamilton is upset that he can't update h...\n",
       "2    @mattycus @Kenichan I dived many times for the...\n",
       "3    @ElleCTF my whole body feels itchy and like it...\n",
       "4    @Karoli @nationwideclass no, it's not behaving...\n",
       "5              @joy_wolf @Kwesidei not the whole crew \n",
       "6                                 @mybirch Need a hug \n",
       "7    @coZZ @LOLTrish hey  long time no see! Yes.. R...\n",
       "8    @2Hood4Hollywood @Tatiana_K nope they didn't h...\n",
       "9                   @mimismo @twittera que me muera ? \n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '@' + tweets.loc[:,'user'] + ' ' + tweets.loc[:,'tweet']\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-express",
   "metadata": {},
   "source": [
    "Define attention mask and token types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "light-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising users ... 1600000\n",
      "Tokenising tweets ... 1600000\n",
      "Saving to pre-cached : /Users/pataoengineer/data/tweets/_cache/bert-tokenised.pkl\n"
     ]
    }
   ],
   "source": [
    "if 'token_user' not in tweets.columns:\n",
    "    tokenise = np.vectorize(mbertTokeniser.encode)\n",
    "    nrecords = len(tweets)\n",
    "    print(f'Tokenising users ... {nrecords}')\n",
    "    tweets.loc[:,'token_user'] = tweets.loc[:,'user'].apply(tokenise)\n",
    "    print(f'Tokenising tweets ... {nrecords}')\n",
    "    tweets.loc[:,'token_tweet'] = tweets.loc[:,'tweet'].apply(tokenise)\n",
    "    print(f'Saving to pre-cached : {path_cache}')\n",
    "    tweets.to_pickle(path_cache)\n",
    "else:\n",
    "    print('Skipping tokenisation, already pre-computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "green-trader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_TheSpecialOne_'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['user'] # original user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bridal-expansion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '_', 'the', '##sp', '##ec', '##ial', '##one', '_', '[SEP]']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertTokeniser.convert_ids_to_tokens(tweets.iloc[0]['token_user']) # token user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "governmental-heater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'switch',\n",
       " '##foot',\n",
       " 'http',\n",
       " ':',\n",
       " '/',\n",
       " '/',\n",
       " 't',\n",
       " '##wi',\n",
       " '##tp',\n",
       " '##ic',\n",
       " '.',\n",
       " 'com',\n",
       " '/',\n",
       " '2',\n",
       " '##y',\n",
       " '##1',\n",
       " '##z',\n",
       " '##l',\n",
       " '-',\n",
       " 'aw',\n",
       " '##w',\n",
       " '##w',\n",
       " ',',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'bum',\n",
       " '##mer',\n",
       " '.',\n",
       " 'you',\n",
       " 'should',\n",
       " '##a',\n",
       " 'got',\n",
       " 'david',\n",
       " 'carr',\n",
       " 'of',\n",
       " 'third',\n",
       " 'day',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " '.',\n",
       " ';',\n",
       " 'd',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertTokeniser.convert_ids_to_tokens(tweets.iloc[0]['token_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "printable-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max length of vector\n",
    "def get_len(tw):\n",
    "    return len(tw)\n",
    "max(tweets['token_tweet'].apply(get_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-vertex",
   "metadata": {},
   "source": [
    "Concatenate user tokens and tweet tokens altogether. \n",
    "Also pad the length and record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "mysterious-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "def concat_ids(u, t):\n",
    "    return np.hstack([u,t[1:]]) # exclude 1st [CLS] from `t`\n",
    "\n",
    "tweets.loc[:,'ids'] = tweets.apply(\n",
    "    lambda row: concat_ids(row['token_user'], row['token_tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "visible-serve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataoengineer/opt/miniconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='len_ids'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets.loc[:,'len_ids'] = tweets['ids'].apply(len)\n",
    "sns.boxplot(tweets['len_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "accompanied-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad ids with zeros so all have same length\n",
    "tweets.loc[:,'ids'] = tweets.loc[:,'ids'].apply(lambda ids: \\\n",
    "    np.pad(ids, (0, MAX_LEN-len(ids)), constant_values=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "equipped-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataoengineer/opt/miniconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='ids'>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(tweets['ids'].apply(len)) # lengths after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "similar-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataoengineer/opt/miniconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='attention_mask'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets.loc[:,'attention_mask'] = tweets['len_ids'].apply(lambda ld: [1]*ld + [0]*(MAX_LEN-ld))\n",
    "tweets.loc[:,'token_type'] = tweets['len_ids'].apply(lambda ld: [0]*MAX_LEN)\n",
    "\n",
    "sns.boxplot(tweets['attention_mask'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "continued-farmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment          int64\n",
       "user              object\n",
       "tweet             object\n",
       "token_user        object\n",
       "token_tweet       object\n",
       "ids               object\n",
       "len_ids            int64\n",
       "attention_mask    object\n",
       "token_type        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-studio",
   "metadata": {},
   "source": [
    "# 3. Train first model\n",
    "for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "informal-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "protective-litigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "greenhouse-hotel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120000, 480000)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(tweets, test_size=0.3)\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-obligation",
   "metadata": {},
   "source": [
    "Train BERT-based classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "removable-glucose",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m           \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    479\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0;32m--> 480\u001b[0;31m                   (element, type(element).__name__))\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for 1429431    [101, 2540, 3619, 2692, 2571, 102, 8299, 1024,...\n759260     [101, 1056, 14642, 11451, 2239, 102, 8299, 102...\n1155588    [101, 3335, 20110, 2050, 102, 1030, 24537, 277...\n882136     [101, 7059, 17134, 23833, 102, 18168, 2290, 21...\n1523176    [101, 7842, 25708, 7646, 2121, 22025, 2549, 10...\n                                 ...                        \n1360738    [101, 14418, 14268, 6305, 102, 5667, 1045, 222...\n625479     [101, 7632, 3900, 11607, 9397, 22787, 2594, 10...\n1535171    [101, 21772, 11239, 7770, 102, 1030, 5180, 109...\n193340     [101, 14348, 25766, 102, 1030, 8038, 11335, 25...\n1270056    [101, 6616, 6499, 6806, 102, 1030, 2720, 1035,...\nName: ids, Length: 1120000, dtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-a8e97b0f64d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     batch_size=100)\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    380\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     ))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \"\"\"\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3136\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3138\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    [train['ids'], train['attention_mask'], train['token_type']],\n",
    "    train['sentiment'],\n",
    "    epochs=5,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-pride",
   "metadata": {},
   "source": [
    "## Evaluate model (offline)\n",
    "with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-prescription",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-links",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
