{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "million-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-madness",
   "metadata": {},
   "source": [
    "# 1. Load BERT Model\n",
    "\n",
    "Pretrained multi-lingual from HuggingFace\n",
    "\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "random-latitude",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Multi-lingual\n",
    "mbertTokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mbertModel = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mbertModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diverse-omaha",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mbertModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-czech",
   "metadata": {},
   "source": [
    "## Test encoding text in various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "brave-semiconductor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[ 101, 2026, 2047, 2338, 2003, 2417,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'My new book is red',\n",
    "    'The journal has been approved by the dean!',\n",
    "    'It was a dangerous explosion caused by chemical reaction',\n",
    "    'Potassium Nitrate'\n",
    "]\n",
    "\n",
    "tensors = [mbertTokeniser(s, return_tensors='tf') for s in sentences]\n",
    "\n",
    "tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "attended-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "wouts = [mbertModel(t) for t in tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "quantitative-hunger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'pooler_output']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wouts[0]] # output structure of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "proper-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 7, 768), (1, 11, 768), (1, 11, 768), (1, 4, 768)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w[0].numpy().shape for w in wouts] # sequence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "apparent-violation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 768), (1, 768), (1, 768), (1, 768)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w[1].numpy().shape for w in wouts] # CLS embedding (pooled) output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "demographic-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 768), (1, 768), (1, 768), (1, 768)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten hidden states of N words in each sentence\n",
    "# by averaging\n",
    "fseqs = [w[1].numpy() for w in wouts]\n",
    "list(map(lambda f: f.shape, fseqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "worse-cleveland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'do', 'you', 'know', 'who', 'i', 'am', '[SEP]']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = mbertTokeniser.encode('Do you know who i am') # str -> ids\n",
    "mbertTokeniser.convert_ids_to_tokens(encoded) # ids -> tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-particle",
   "metadata": {},
   "source": [
    "Distance between each pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "stunning-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "zs = list(zip(sentences, fseqs))\n",
    "\n",
    "closest = []\n",
    "for i,(s1,v1) in enumerate(zs):\n",
    "    for j,(s2,v2) in enumerate(zs[i+1:]):\n",
    "        c = cosine_similarity(\n",
    "            np.atleast_2d(v1.flatten()),\n",
    "            np.atleast_2d(v2.flatten()))\n",
    "        heappush(closest, (-c, (s1,s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "detailed-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 closest pairs\n",
      "==========================\n",
      "Rank #0, [[0.97097844]]\n",
      "It was a dangerous explosion caused by chemical reaction\n",
      "Potassium Nitrate\n",
      "==========================\n",
      "Rank #1, [[0.9686246]]\n",
      "The journal has been approved by the dean!\n",
      "It was a dangerous explosion caused by chemical reaction\n",
      "==========================\n",
      "Rank #2, [[0.9531851]]\n",
      "The journal has been approved by the dean!\n",
      "Potassium Nitrate\n",
      "==========================\n",
      "Rank #3, [[0.9341798]]\n",
      "My new book is red\n",
      "The journal has been approved by the dean!\n",
      "==========================\n",
      "Rank #4, [[0.8713379]]\n",
      "My new book is red\n",
      "It was a dangerous explosion caused by chemical reaction\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 closest pairs')\n",
    "for i in range(5):\n",
    "    c, (s1,s2) = heappop(closest)\n",
    "    print('==========================')\n",
    "    print(f'Rank #{i}, {-c}')\n",
    "    print(s1)\n",
    "    print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-industry",
   "metadata": {},
   "source": [
    "These sentence similarities make no sense at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-farming",
   "metadata": {},
   "source": [
    "## Build Classification based on BERT\n",
    "\n",
    "By freezing pretrained layer of BERT, and add a new softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "rapid-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbertModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "disciplinary-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7fb8d3b0a290>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-level",
   "metadata": {},
   "source": [
    "Add custom smoothing & classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "heard-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "affected-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer (1) taking tokenised words\n",
    "input_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,), \n",
    "    dtype=tf.int32,\n",
    "    name=\"input_ids\")\n",
    "\n",
    "# input layer (2) taking attention masks (masking paddings)\n",
    "mask_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,),\n",
    "    dtype=tf.int32,\n",
    "    name=\"attention_mask_ids\")\n",
    "\n",
    "# input layer (3) taking token types\n",
    "token_type_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,),\n",
    "    dtype=tf.int32,\n",
    "    name=\"token_type_ids\")\n",
    "\n",
    "# BERT layer\n",
    "outputs = mbertModel(\n",
    "    input_ids,\n",
    "    attention_mask=mask_ids,\n",
    "    token_type_ids=token_type_ids)\n",
    "\n",
    "# smoothening layers\n",
    "dense = keras.layers.Dense(768, name=\"dense\")(outputs[1]) # feed BERT's pooled output\n",
    "dropout = keras.layers.Dropout(0.2, name=\"dropout\")(dense)\n",
    "norm = keras.layers.LayerNormalization(name=\"norm\")(dropout)\n",
    "logit = keras.layers.Dense(1, activation='sigmoid', name=\"logit\")(norm)\n",
    "\n",
    "# Modeling\n",
    "model = keras.models.Model(\n",
    "    inputs=[input_ids, mask_ids, token_type_ids],\n",
    "    outputs=[logit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "aggressive-footwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_ids (InputLayer) [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask_ids[0][0]         \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 768)          590592      tf_bert_model_1[22][1]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm (LayerNormalization)       (None, 768)          1536        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "logit (Dense)                   (None, 1)            769         norm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 110,075,137\n",
      "Trainable params: 592,897\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "enormous-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-looking",
   "metadata": {},
   "source": [
    "## 2. Load tweets data\n",
    "For classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "requested-efficiency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                              tweet\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....\n",
       "5         joy_wolf                      @Kwesidei not the whole crew \n",
       "6          mybirch                                        Need a hug \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it \n",
       "9          mimismo                          @twittera que me muera ? "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tweets = os.path.join(os.environ['HOME'], 'data', 'tweets', 'training.1600000.processed.noemoticon.csv')\n",
    "cols = ['user','tweet']\n",
    "tweets = pd.read_csv(path_tweets, usecols=[4,5], header=None, index_col=None)\n",
    "tweets.columns = cols\n",
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-martin",
   "metadata": {},
   "source": [
    "Prepare text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "grateful-negative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @_TheSpecialOne_ @switchfoot http://twitpic.co...\n",
       "1    @scotthamilton is upset that he can't update h...\n",
       "2    @mattycus @Kenichan I dived many times for the...\n",
       "3    @ElleCTF my whole body feels itchy and like it...\n",
       "4    @Karoli @nationwideclass no, it's not behaving...\n",
       "5              @joy_wolf @Kwesidei not the whole crew \n",
       "6                                 @mybirch Need a hug \n",
       "7    @coZZ @LOLTrish hey  long time no see! Yes.. R...\n",
       "8    @2Hood4Hollywood @Tatiana_K nope they didn't h...\n",
       "9                   @mimismo @twittera que me muera ? \n",
       "dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '@' + tweets.loc[:,'user'] + ' ' + tweets.loc[:,'tweet']\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-wholesale",
   "metadata": {},
   "source": [
    "Define attention mask and token types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "improving-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise = np.vectorize(mbertTokeniser.encode)\n",
    "\n",
    "# Tokenised process takes long, lets cache\n",
    "path_cache = os.path.join(os.environ['HOME'], 'data', 'tweets', '_cache', 'bert-tokenised.pkl')\n",
    "if os.path.isfile(path_cache):\n",
    "    print(f'Loading pre-cached : {path_cache}')\n",
    "    tweets = pd.read_pickle(path_cache)\n",
    "else:\n",
    "    nrecords = len(tweets)\n",
    "    print(f'Tokenising users ... {nrecords}')\n",
    "    tweets.loc[:,'token_user'] = tweets.loc[:,'user'].apply(tokenise)\n",
    "    print(f'Tokenising tweets ... {nrecords}')\n",
    "    tweets.loc[:,'token_tweet'] = tweets.loc[:,'tweet'].apply(tokenise)\n",
    "    print(f'Saving to pre-cached : {path_cache}')\n",
    "    tweets.to_pickle(path_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "western-handle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_TheSpecialOne_'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['user'] # original user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "mediterranean-syria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '_', 'the', '##sp', '##ec', '##ial', '##one', '_', '[SEP]']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertTokeniser.convert_ids_to_tokens(tweets.iloc[0]['token_user']) # token user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "induced-circulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'switch',\n",
       " '##foot',\n",
       " 'http',\n",
       " ':',\n",
       " '/',\n",
       " '/',\n",
       " 't',\n",
       " '##wi',\n",
       " '##tp',\n",
       " '##ic',\n",
       " '.',\n",
       " 'com',\n",
       " '/',\n",
       " '2',\n",
       " '##y',\n",
       " '##1',\n",
       " '##z',\n",
       " '##l',\n",
       " '-',\n",
       " 'aw',\n",
       " '##w',\n",
       " '##w',\n",
       " ',',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'bum',\n",
       " '##mer',\n",
       " '.',\n",
       " 'you',\n",
       " 'should',\n",
       " '##a',\n",
       " 'got',\n",
       " 'david',\n",
       " 'carr',\n",
       " 'of',\n",
       " 'third',\n",
       " 'day',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " '.',\n",
       " ';',\n",
       " 'd',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertTokeniser.convert_ids_to_tokens(tweets.iloc[0]['token_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "perfect-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max length of vector\n",
    "def get_len(tw):\n",
    "    return len(tw)\n",
    "max(tweets['token_tweet'].apply(get_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-failure",
   "metadata": {},
   "source": [
    "Compose embeddings for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['token_tweet'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "hollow-syria",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-e5d145f23a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m tweets.loc[:,'ids'] = tweets.apply(\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconcat_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_user'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     axis=1)\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7766\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7767\u001b[0m         )\n\u001b[0;32m-> 7768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-e5d145f23a67>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m tweets.loc[:,'ids'] = tweets.apply(\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconcat_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_user'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     axis=1)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2106\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2186\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m             \u001b[0;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-e5d145f23a67>\u001b[0m in \u001b[0;36mconcat_ids\u001b[0;34m(u, t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m165\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconcat_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpadder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpadder\u001b[0m \u001b[0;31m# exclude 1st [CLS] from `t`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconcat_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 165\n",
    "def concat_ids(u, t):\n",
    "    padder = MAX_LEN - len(u) - len(t[1:])\n",
    "    return u + t[1:] + [0]*padder # exclude 1st [CLS] from `t`\n",
    "concat_ids = np.vectorize(concat_ids)\n",
    "\n",
    "tweets.loc[:,'ids'] = tweets.apply(\n",
    "    lambda row: concat_ids(row['token_user'], row['token_tweet']),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-municipality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
