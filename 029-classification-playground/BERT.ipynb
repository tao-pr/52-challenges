{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "million-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-madness",
   "metadata": {},
   "source": [
    "# 1. Load BERT Model\n",
    "\n",
    "Pretrained multi-lingual from HuggingFace\n",
    "\n",
    "https://huggingface.co/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "random-latitude",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Multi-lingual\n",
    "mbertTokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mbertModel = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mbertModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-omaha",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mbertModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-albert",
   "metadata": {},
   "source": [
    "## Test encoding text in various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appreciated-gabriel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[ 101, 2026, 2047, 2338, 2003, 2417,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'My new book is red',\n",
    "    'The journal has been approved by the dean!',\n",
    "    'It was a dangerous explosion caused by chemical reaction',\n",
    "    'Potassium Nitrate'\n",
    "]\n",
    "\n",
    "tensors = [mbertTokeniser(s, return_tensors='tf') for s in sentences]\n",
    "\n",
    "tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aerial-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "wouts = [mbertModel(t) for t in tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blank-builder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'pooler_output']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wouts[0]] # output structure of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "civilian-phrase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 7, 768), (1, 11, 768), (1, 11, 768), (1, 4, 768)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w[0].numpy().shape for w in wouts] # sequence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assisted-fiber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 768), (1, 768), (1, 768), (1, 768)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w[1].numpy().shape for w in wouts] # CLS embedding (pooled) output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wireless-technical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 768), (1, 768), (1, 768), (1, 768)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten hidden states of N words in each sentence\n",
    "# by averaging\n",
    "fseqs = [w[1].numpy() for w in wouts]\n",
    "list(map(lambda f: f.shape, fseqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accurate-trail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'do', 'you', 'know', 'who', 'i', 'am', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = mbertTokeniser.encode('Do you know who i am') # str -> ids\n",
    "mbertTokeniser.convert_ids_to_tokens(encoded) # ids -> tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-reporter",
   "metadata": {},
   "source": [
    "Distance between each pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "solved-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "zs = list(zip(sentences, fseqs))\n",
    "\n",
    "closest = []\n",
    "for i,(s1,v1) in enumerate(zs):\n",
    "    for j,(s2,v2) in enumerate(zs[i+1:]):\n",
    "        c = cosine_similarity(\n",
    "            np.atleast_2d(v1.flatten()),\n",
    "            np.atleast_2d(v2.flatten()))\n",
    "        heappush(closest, (-c, (s1,s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "friendly-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 closest pairs\n",
      "==========================\n",
      "Rank #0, [[0.97097844]]\n",
      "It was a dangerous explosion caused by chemical reaction\n",
      "Potassium Nitrate\n",
      "==========================\n",
      "Rank #1, [[0.9686246]]\n",
      "The journal has been approved by the dean!\n",
      "It was a dangerous explosion caused by chemical reaction\n",
      "==========================\n",
      "Rank #2, [[0.9531851]]\n",
      "The journal has been approved by the dean!\n",
      "Potassium Nitrate\n",
      "==========================\n",
      "Rank #3, [[0.9341798]]\n",
      "My new book is red\n",
      "The journal has been approved by the dean!\n",
      "==========================\n",
      "Rank #4, [[0.8713379]]\n",
      "My new book is red\n",
      "It was a dangerous explosion caused by chemical reaction\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 closest pairs')\n",
    "for i in range(5):\n",
    "    c, (s1,s2) = heappop(closest)\n",
    "    print('==========================')\n",
    "    print(f'Rank #{i}, {-c}')\n",
    "    print(s1)\n",
    "    print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-marketplace",
   "metadata": {},
   "source": [
    "These sentence similarities make no sense at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-florida",
   "metadata": {},
   "source": [
    "## Build Classification based on BERT\n",
    "\n",
    "By freezing pretrained layer of BERT, and add a new softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collect-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbertModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "received-justice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7ffe0d869f50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-operator",
   "metadata": {},
   "source": [
    "Add custom smoothing & classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aware-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "tropical-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer (1) taking tokenised words\n",
    "input_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,), \n",
    "    dtype=tf.int32,\n",
    "    name=\"input_ids\")\n",
    "\n",
    "# input layer (2) taking attention masks (masking paddings)\n",
    "mask_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,),\n",
    "    dtype=tf.int32,\n",
    "    name=\"attention_mask_ids\")\n",
    "\n",
    "# input layer (3) taking token types\n",
    "token_type_ids = keras.layers.Input(\n",
    "    shape=(MAX_LEN,),\n",
    "    dtype=tf.int32,\n",
    "    name=\"token_type_ids\")\n",
    "\n",
    "# BERT layer, shape (768,)\n",
    "outputs = mbertModel(\n",
    "    input_ids,\n",
    "    attention_mask=mask_ids,\n",
    "    token_type_ids=token_type_ids)\n",
    "\n",
    "# smoothening layers\n",
    "logit = keras.layers.Dense(64, name=\"dense\")(outputs[1]) # feed BERT's pooled output\n",
    "logit = keras.layers.Flatten()(logit)\n",
    "logit = keras.layers.Dropout(0.2, name=\"dropout\")(logit)\n",
    "\n",
    "act = keras.layers.Dense(\n",
    "    1, name='softmax',\n",
    "    activation='softmax',\n",
    "    kernel_regularizer=tf.keras.regularizers.L2(0.01))(logit)\n",
    "\n",
    "# Modeling\n",
    "model = keras.models.Model(\n",
    "    inputs=[input_ids, mask_ids, token_type_ids],\n",
    "    outputs=[act],\n",
    "    name='BERT-clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "integral-myrtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT-clf\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_ids (InputLayer) [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask_ids[0][0]         \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       tf_bert_model[6][1]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 1)            65          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,531,521\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "supposed-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5), # use low LR for BERT\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "approved-stevens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 150) dtype=int32 (created by layer 'input_ids')>,\n",
       " <KerasTensor: shape=(None, 150) dtype=int32 (created by layer 'attention_mask_ids')>,\n",
       " <KerasTensor: shape=(None, 150) dtype=int32 (created by layer 'token_type_ids')>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "institutional-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'softmax')>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-pregnancy",
   "metadata": {},
   "source": [
    "## 2. Load tweets data\n",
    "For classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "widespread-flight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-cached : /Users/pataoengineer/data/tweets/_cache/bert-tokenised.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load pre-cached if exists\n",
    "path_cache = os.path.join(os.environ['HOME'], 'data', 'tweets', '_cache', 'bert-tokenised.pkl')\n",
    "if os.path.isfile(path_cache):\n",
    "    print(f'Loading pre-cached : {path_cache}')\n",
    "    tweets = pd.read_pickle(path_cache)\n",
    "else:\n",
    "    path_tweets = os.path.join(os.environ['HOME'], 'data', 'tweets', 'training.1600000.processed.noemoticon.csv')\n",
    "    cols = ['sentiment','user','tweet']\n",
    "    tweets = pd.read_csv(path_tweets, usecols=[0,4,5], header=None, index_col=None)\n",
    "    tweets.columns = cols\n",
    "    \n",
    "    # Take only small subset\n",
    "    pos = tweets[tweets['sentiment']==4].sample(n=10000, replace=False)\n",
    "    neg = tweets[tweets['sentiment']==0].sample(n=10000, replace=False)\n",
    "    tweets = pd.concat([pos,neg])\n",
    "    tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-multimedia",
   "metadata": {},
   "source": [
    "Define attention mask and token types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "steady-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping tokenisation, already pre-computed\n"
     ]
    }
   ],
   "source": [
    "if 'token_user' not in tweets.columns:\n",
    "    def tokenise(w):\n",
    "        return mbertTokeniser.encode(w)\n",
    "    tokenise = np.vectorize(tokenise)\n",
    "    nrecords = len(tweets)\n",
    "    print(f'Tokenising users ... {nrecords}')\n",
    "    tweets.loc[:,'token_user'] = tweets.loc[:,'user'].apply(tokenise)\n",
    "    print(f'Tokenising tweets ... {nrecords}')\n",
    "    tweets.loc[:,'token_tweet'] = tweets.loc[:,'tweet'].apply(tokenise)\n",
    "    print(f'Saving to pre-cached : {path_cache}')\n",
    "    tweets.to_pickle(path_cache)\n",
    "else:\n",
    "    print('Skipping tokenisation, already pre-computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "closing-casting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CMGeekCrissa'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['user'] # original user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stock-attachment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS],cm,##gee,##k,##cr,##issa,[SEP]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(\n",
    "    mbertTokeniser.convert_ids_to_tokens(tweets.iloc[0]['token_user']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "traditional-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS],@,ad,##ych,##ou,morning,ad,##y,[SEP]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(\n",
    "    mbertTokeniser.convert_ids_to_tokens(tweets.iloc[0]['token_tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "least-toolbox",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find max length of vector\n",
    "def get_len(tw):\n",
    "    return len(tw)\n",
    "max(tweets['token_tweet'].apply(get_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-savings",
   "metadata": {},
   "source": [
    "Concatenate user tokens and tweet tokens altogether. \n",
    "Also pad the length and record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "thirty-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_ids(u, t):\n",
    "    # exclude 1st [CLS] from `t`\n",
    "    return u.tolist() + t[1:].tolist()\n",
    "\n",
    "tweets.loc[:,'ids'] = tweets.apply(\n",
    "    lambda row: concat_ids(row['token_user'], row['token_tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "superior-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataoengineer/opt/miniconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='len_ids'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets.loc[:,'len_ids'] = tweets['ids'].apply(len)\n",
    "sns.boxplot(tweets['len_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "general-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad ids with zeros so all have same length\n",
    "tweets.loc[:,'ids'] = tweets.loc[:,'ids'].apply(lambda ids: \\\n",
    "    ids + [0]*(MAX_LEN-len(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "excess-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pataoengineer/opt/miniconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='ids'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(tweets['ids'].apply(len)) # lengths after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "transparent-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[:,'attention_mask'] = tweets['len_ids'].apply(\n",
    "    lambda ld: [1]*ld + [0]*(MAX_LEN-ld))\n",
    "tweets.loc[:,'token_type'] = tweets['len_ids'].apply(\n",
    "    lambda ld: [1]*ld + [0]*(MAX_LEN-ld))\n",
    "\n",
    "#sns.boxplot(tweets['attention_mask'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "moved-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'list'>, <class 'list'>, <class 'list'>]\n",
      "[150, 150, 150]\n"
     ]
    }
   ],
   "source": [
    "cols = ['ids','attention_mask','token_type']\n",
    "r = tweets.iloc[0][cols]\n",
    "print([type(r[c]) for c in cols])\n",
    "print([len(r[c]) for c in cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-evidence",
   "metadata": {},
   "source": [
    "# 3. Train first model\n",
    "for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "existing-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "neural-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-miller",
   "metadata": {},
   "source": [
    "## Prepare dataset for TF\n",
    "Dirty way. Putting all numpy vectors into ragged tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-convert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-lafayette",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-acrobat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "electronic-plumbing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 6000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(tweets, test_size=0.3)\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "liked-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tf.convert_to_tensor(train['ids'].values.tolist())\n",
    "mask = tf.convert_to_tensor(train['attention_mask'].values.tolist())\n",
    "types = tf.convert_to_tensor(train['token_type'].values.tolist())\n",
    "labels = tf.convert_to_tensor(train['sentiment'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "forty-maple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([14000, 150]),\n",
       " TensorShape([14000, 150]),\n",
       " TensorShape([14000, 150]),\n",
       " TensorShape([14000]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape, mask.shape, types.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-footage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "historical-croatia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 150) dtype=int32 (created by layer 'input_ids')>,\n",
       " <KerasTensor: shape=(None, 150) dtype=int32 (created by layer 'attention_mask_ids')>,\n",
       " <KerasTensor: shape=(None, 150) dtype=int32 (created by layer 'token_type_ids')>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "right-marshall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'softmax')>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-mobile",
   "metadata": {},
   "source": [
    "Train BERT-based classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "conservative-convenience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 2650s 38s/step - loss: 5.0253 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 2561s 37s/step - loss: 4.9667 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 2552s 36s/step - loss: 5.0148 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 2508s 36s/step - loss: 5.0011 - accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 2486s 36s/step - loss: 5.0164 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffde458a7d0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    [ids, mask, types],\n",
    "    labels,\n",
    "    epochs=5,\n",
    "    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-costa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "underlying-spirit",
   "metadata": {},
   "source": [
    "## Evaluate model (offline)\n",
    "with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-drunk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-egypt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
